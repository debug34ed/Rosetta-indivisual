{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "better-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "looking-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# # This method of data loading is not suitable for static graph in TF1.14, \n",
    "# # you can run this block in TF1.14 eager mode to use iterable objects.\n",
    "# # If one day Rosetta support TF dynamic graph, these lines count.\n",
    "\n",
    "\n",
    "# train_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
    "# train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url), origin=train_dataset_url)\n",
    "# print(\"Local copy of the dataset file: {}\".format(train_dataset_fp))\n",
    "\n",
    "# test_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\"\n",
    "# test_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(test_dataset_url), origin=test_dataset_url)\n",
    "# print(\"Local copy of the dataset file: {}\".format(test_dataset_fp))\n",
    "\n",
    "# column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "# feature_names = column_names[:-1]\n",
    "# label_name = column_names[-1]\n",
    "# print(\"Features: {}\".format(feature_names))\n",
    "# print(\"Label: {}\".format(label_name))\n",
    "# class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']\n",
    "\n",
    "# batch_size = 32\n",
    "# train_dataset = tf.data.experimental.make_csv_dataset(\n",
    "#     train_dataset_fp,\n",
    "#     batch_size,\n",
    "#     column_names=column_names,\n",
    "#     label_name=label_name,\n",
    "#     num_epochs=1)\n",
    "# print(train_dataset)\n",
    "# test_dataset = tf.data.experimental.make_csv_dataset(\n",
    "#     test_dataset_fp,\n",
    "#     batch_size,\n",
    "#     column_names=column_names,\n",
    "#     label_name=label_name,\n",
    "#     num_epochs=1,\n",
    "#     shuffle=False)\n",
    "\n",
    "# def pack_features_vector(features, labels):\n",
    "#     \"\"\"Pack the features into a single array.\"\"\"\n",
    "#     features = tf.stack(list(features.values()), axis=1)\n",
    "#     return features, labels\n",
    "\n",
    "# train_dataset = train_dataset.map(pack_features_vector)\n",
    "# test_dataset = test_dataset.map(pack_features_vector)\n",
    "\n",
    "# iter_train_dataset = iter(train_dataset)\n",
    "# iter_test_dataset = iter(test_dataset)\n",
    "\n",
    "# train_features, train_labels = next(iter_train_dataset)\n",
    "# test_features, test_labels = next(iter_test_dataset)\n",
    "\n",
    "# print(train_features, train_labels)\n",
    "# print(test_features, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "parliamentary-runner",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Local copy of the dataset file: /home/juzix/.keras/datasets/iris_training.csv\n",
      "Local copy of the dataset file: /home/juzix/.keras/datasets/iris_test.csv\n",
      "/home/juzix/prRosetta/Rosetta/example/contrib\n",
      "/home/juzix/prRosetta/Rosetta/example/tutorials\n",
      "/home/juzix/prRosetta/Rosetta/example/contrib\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# This block is to split the data into features and label cols, like tutorials, \n",
    "# you can copy this block into a new script, don't forget to import tensorflow, numpy and pandas\n",
    "\n",
    "train_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
    "train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url), origin=train_dataset_url)\n",
    "print(\"Local copy of the dataset file: {}\".format(train_dataset_fp))\n",
    "\n",
    "test_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\"\n",
    "test_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(test_dataset_url), origin=test_dataset_url)\n",
    "print(\"Local copy of the dataset file: {}\".format(test_dataset_fp))\n",
    "\n",
    "!pwd\n",
    "# Copy Iris dataset into Rosetta dir for different party(ALL, PO or P1)\n",
    "# !cp /home/username/.keras/datasets/iris_training.csv /path_to_Rosetta/example/tutorials/dsets/ALL/\n",
    "# !cp /home/username/.keras/datasets/iris_test.csv /path_to_Rosetta/example/tutorials/dsets/ALL/\n",
    "\n",
    "tutorials_path = os.path.abspath(os.path.join(os.getcwd(), \"../tutorials/\"))\n",
    "print(tutorials_path)\n",
    "print(os.getcwd())\n",
    "\n",
    "iris_training_features = pd.read_csv(tutorials_path + \"/dsets/ALL/iris_training.csv\", \\\n",
    "    skiprows=[0], usecols=[0, 1, 2, 3])\n",
    "iris_training_label = pd.read_csv(tutorials_path + \"/dsets/ALL/iris_training.csv\", \\\n",
    "    skiprows=[0], usecols=[4])\n",
    "iris_test_features = pd.read_csv(tutorials_path + \"/dsets/ALL/iris_test.csv\", \\\n",
    "    skiprows=[0], usecols=[0, 1, 2, 3])\n",
    "iris_test_label = pd.read_csv(tutorials_path + \"/dsets/ALL/iris_test.csv\", \\\n",
    "    skiprows=[0], usecols=[4])\n",
    "# print(iris_training_features)\n",
    "# print(iris_training_label)\n",
    "# print(iris_test_features)\n",
    "# print(iris_test_label)\n",
    "\n",
    "iris_training_features.to_csv(tutorials_path + \"/dsets/ALL/iris_training_features.csv\", index=0)\n",
    "iris_training_label.to_csv(tutorials_path + \"/dsets/ALL/iris_training_label.csv\", index=0)\n",
    "iris_test_features.to_csv(tutorials_path + \"/dsets/ALL/iris_test_features.csv\", index=0)\n",
    "iris_test_label.to_csv(tutorials_path + \"/dsets/ALL/iris_test_label.csv\", index=0)\n",
    "\n",
    "\n",
    "\n",
    "# # For P0 and P1 in RTT version, they should do some preprocess on label.\n",
    "# iris_training_label_P = pd.read_csv(tutorials_path + \"/dsets/P1/iris_training_label.csv\", header=None).to_numpy() - 1\n",
    "# iris_test_label_P = pd.read_csv(tutorials_path + \"/dsets/P1/iris_test_label.csv\", header=None).to_numpy() - 1\n",
    "# \n",
    "# print(iris_training_label_P, iris_training_label_P.shape)\n",
    "# print(iris_test_label_P, iris_test_label_P.shape)\n",
    "# \n",
    "# iris_training_label_P = pd.DataFrame(iris_training_label_P)\n",
    "# iris_test_label_P = pd.DataFrame(iris_test_label_P)\n",
    "# \n",
    "# print(iris_training_label_P)\n",
    "# print(iris_test_label_P)\n",
    "# \n",
    "# iris_training_label_P.to_csv(tutorials_path + \"/dsets/P1/iris_training_label.csv\", index=False, header=False)\n",
    "# iris_test_label_P.to_csv(tutorials_path + \"/dsets/P1/iris_test_label.csv\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cutting-destination",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 4), dtype=float32)\n",
      "Tensor(\"Placeholder_1:0\", shape=(?, 1), dtype=float32)\n",
      "<tf.Variable 'Variable:0' shape=(4, 1) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_1:0' shape=(1,) dtype=float32_ref>\n",
      "Tensor(\"add:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "name: \"GradientDescent\"\n",
      "op: \"NoOp\"\n",
      "input: \"^GradientDescent/update_Variable/ApplyGradientDescent\"\n",
      "input: \"^GradientDescent/update_Variable_1/ApplyGradientDescent\"\n",
      "\n",
      "name: \"init\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Variable/Assign\"\n",
      "input: \"^Variable_1/Assign\"\n",
      "\n",
      "init weight:[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "bias:[0.]\n",
      "I,E,B:0000,0000,0000 weight:[[ 0.004875  ]\n",
      " [-0.0001125 ]\n",
      " [ 0.0099    ]\n",
      " [ 0.00395625]] \n",
      "bias:[0.000375]\n",
      "I,E,B:0003,0000,0003 weight:[[-0.00494406]\n",
      " [-0.01271045]\n",
      " [ 0.02192803]\n",
      " [ 0.01086732]] \n",
      "bias:[-0.00313594]\n",
      "Y_comp:\n",
      " [[ 0.03796146  0.        ]\n",
      " [ 0.0645804   1.        ]\n",
      " [-0.02758382 -1.        ]\n",
      " [ 0.04004803  0.        ]\n",
      " [ 0.03973525  0.        ]\n",
      " [ 0.03776864  0.        ]\n",
      " [-0.05083947 -1.        ]\n",
      " [ 0.05826116  1.        ]\n",
      " [ 0.03507842  0.        ]\n",
      " [ 0.0787065   1.        ]\n",
      " [ 0.07582664  1.        ]\n",
      " [-0.03731919 -1.        ]\n",
      " [ 0.06277007  1.        ]\n",
      " [ 0.04227734  0.        ]\n",
      " [ 0.03781616  0.        ]\n",
      " [-0.03996452 -1.        ]\n",
      " [ 0.02325117  0.        ]\n",
      " [-0.02624624 -1.        ]\n",
      " [-0.03650067 -1.        ]\n",
      " [ 0.06828159  1.        ]\n",
      " [-0.03359829 -1.        ]\n",
      " [ 0.0460231   0.        ]\n",
      " [ 0.07121432  1.        ]\n",
      " [ 0.0576887   0.        ]\n",
      " [ 0.0324304   0.        ]\n",
      " [ 0.04465713  0.        ]\n",
      " [-0.04697912 -1.        ]\n",
      " [ 0.04369918  0.        ]\n",
      " [ 0.07395244  1.        ]\n",
      " [ 0.03677983  0.        ]]\n",
      "[[ 0.  0.]\n",
      " [ 1.  1.]\n",
      " [-1. -1.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [-1. -1.]\n",
      " [ 1.  1.]\n",
      " [ 0.  0.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]\n",
      " [-1. -1.]\n",
      " [ 1.  1.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [-1. -1.]\n",
      " [ 0.  0.]\n",
      " [-1. -1.]\n",
      " [-1. -1.]\n",
      " [ 1.  1.]\n",
      " [-1. -1.]\n",
      " [ 0.  0.]\n",
      " [ 1.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [-1. -1.]\n",
      " [ 0.  0.]\n",
      " [ 1.  1.]\n",
      " [ 0.  0.]]\n",
      "acc:  1.0\n"
     ]
    }
   ],
   "source": [
    "EPOCHES = 1\n",
    "BATCH_SIZE = 32\n",
    "learning_rate = 0.003\n",
    "\n",
    "# real data\n",
    "# ######################################## difference from rosettta\n",
    "tutorials_path = os.path.abspath(os.path.join(os.getcwd(), \"../tutorials/\"))\n",
    "training_features_file_path = tutorials_path + \"/dsets/ALL/iris_training_features.csv\"\n",
    "training_label_file_path = tutorials_path + \"/dsets/ALL/iris_training_label.csv\"\n",
    "training_features = pd.read_csv(training_features_file_path, header=None).to_numpy()\n",
    "training_label = pd.read_csv(training_label_file_path, header=None).to_numpy() - 1\n",
    "test_features_file_path = tutorials_path + \"/dsets/ALL/iris_test_features.csv\"\n",
    "test_label_file_path = tutorials_path + \"/dsets/ALL/iris_test_label.csv\"\n",
    "test_features = pd.read_csv(test_features_file_path, header=None).to_numpy()\n",
    "test_label = pd.read_csv(test_label_file_path, header=None).to_numpy() - 1\n",
    "# ######################################## difference from rosettta\n",
    "DIM_NUM = training_features.shape[1]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, DIM_NUM])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "print(X)\n",
    "print(Y)\n",
    "\n",
    "# initialize W & b\n",
    "W = tf.Variable(tf.zeros([DIM_NUM, 1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "print(W)\n",
    "print(b)\n",
    "\n",
    "# predict\n",
    "pred_Y = tf.matmul(X, W) + b\n",
    "print(pred_Y)\n",
    "\n",
    "# loss\n",
    "loss = tf.square(Y - pred_Y)\n",
    "loss = tf.reduce_mean(loss)\n",
    "print(loss)\n",
    "\n",
    "# optimizer\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "print(train)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "print(init)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    xW, xb = sess.run([W, b])\n",
    "    print(\"init weight:{} \\nbias:{}\".format(xW, xb))\n",
    "\n",
    "    # train\n",
    "    BATCHES = math.ceil(len(training_features) / BATCH_SIZE)\n",
    "    for e in range(EPOCHES):\n",
    "        for i in range(BATCHES):\n",
    "            bX = training_features[(i * BATCH_SIZE): (i + 1) * BATCH_SIZE]\n",
    "            bY = training_label[(i * BATCH_SIZE): (i + 1) * BATCH_SIZE]\n",
    "            sess.run(train, feed_dict={X: bX, Y: bY})\n",
    "\n",
    "            j = e * BATCHES + i\n",
    "            if j % 50 == 0 or (j == EPOCHES * BATCHES - 1 and j % 50 != 0):\n",
    "                xW, xb = sess.run([W, b])\n",
    "                print(\"I,E,B:{:0>4d},{:0>4d},{:0>4d} weight:{} \\nbias:{}\".format(\n",
    "                    j, e, i, xW, xb))\n",
    "\n",
    "    # predict\n",
    "    Y_pred = sess.run(pred_Y, feed_dict={X: test_features, Y: test_label})\n",
    "#     print(\"Y_pred:\\n\", Y_pred)\n",
    "#     print(\"Y_true:\\n\", test_label)\n",
    "    Y_comp = np.hstack((Y_pred,test_label))\n",
    "    print(\"Y_comp:\\n\", Y_comp)\n",
    "    Y_pred_cls0 = np.where(Y_pred >= 0, Y_pred, -1)\n",
    "    Y_pred_cls0 = np.where(Y_pred < 0, Y_pred_cls0, 0)\n",
    "    Y_pred_cls2 = np.where(Y_pred < 0.05, Y_pred, 1)\n",
    "    Y_pred_cls2 = np.where(Y_pred >= 0.05, Y_pred_cls2, 0)\n",
    "    Y_pred_cls012 = Y_pred_cls0 + Y_pred_cls2\n",
    "    Y_comp = np.hstack((Y_pred_cls012, test_label))\n",
    "    print(Y_comp)\n",
    "    acc = len([i for i in Y_pred_cls012 if i in test_label]) / len(test_label)\n",
    "    print('acc: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}